{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\spark-3.3.3-bin-hadoop3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.init()\n",
    "findspark.find()\n",
    "# findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data from csv, show schema and some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"pyspark_demo\") \\\n",
    "      .config(\"spark.sql.execution.pythonUDF.arrow.enabled\", \"true\") \\\n",
    "      .getOrCreate() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Suburb: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Rooms: integer (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Price: integer (nullable = true)\n",
      " |-- Method: string (nullable = true)\n",
      " |-- SellerG: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- Postcode: integer (nullable = true)\n",
      " |-- Bedroom2: integer (nullable = true)\n",
      " |-- Bathroom: integer (nullable = true)\n",
      " |-- Car: integer (nullable = true)\n",
      " |-- Landsize: integer (nullable = true)\n",
      " |-- BuildingArea: integer (nullable = true)\n",
      " |-- YearBuilt: integer (nullable = true)\n",
      " |-- CouncilArea: string (nullable = true)\n",
      " |-- Lattitude: double (nullable = true)\n",
      " |-- Longtitude: double (nullable = true)\n",
      " |-- Regionname: string (nullable = true)\n",
      " |-- Propertycount: integer (nullable = true)\n",
      "\n",
      "+----------+-------------+-----+----+-------+------+-------+---------+--------+--------+--------+--------+---+--------+------------+---------+------------------+---------+----------+--------------------+-------------+\n",
      "|    Suburb|      Address|Rooms|Type|  Price|Method|SellerG|     Date|Distance|Postcode|Bedroom2|Bathroom|Car|Landsize|BuildingArea|YearBuilt|       CouncilArea|Lattitude|Longtitude|          Regionname|Propertycount|\n",
      "+----------+-------------+-----+----+-------+------+-------+---------+--------+--------+--------+--------+---+--------+------------+---------+------------------+---------+----------+--------------------+-------------+\n",
      "|Abbotsford|68 Studley St|    2|   h|   null|    SS| Jellis| 3/9/2016|     2.5|    3067|       2|       1|  1|     126|        null|     null|Yarra City Council| -37.8014|  144.9958|Northern Metropol...|         4019|\n",
      "|Abbotsford| 85 Turner St|    2|   h|1480000|     S| Biggin|3/12/2016|     2.5|    3067|       2|       1|  1|     202|        null|     null|Yarra City Council| -37.7996|  144.9984|Northern Metropol...|         4019|\n",
      "+----------+-------------+-----+----+-------+------+-------+---------+--------+--------+--------+--------+---+--------+------------+---------+------------------+---------+----------+--------------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a dataframe from RDD\n",
    "# there are a few options such as \"header\", \"delimiter\", \"inferSchema\"\n",
    "df = spark.read.options(header=True, inferSchema=True).csv(\"../data/raw/sampledata.csv\")\n",
    "df.printSchema()\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Selecting and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----+----+-------+------+-------+---------+--------+--------+--------+--------+---+--------+------------+---------+------------------+---------+----------+--------------------+-------------+\n",
      "|    Suburb|     Address|Rooms|Type|  Price|Method|SellerG|     Date|Distance|Postcode|Bedroom2|Bathroom|Car|Landsize|BuildingArea|YearBuilt|       CouncilArea|Lattitude|Longtitude|          Regionname|Propertycount|\n",
      "+----------+------------+-----+----+-------+------+-------+---------+--------+--------+--------+--------+---+--------+------------+---------+------------------+---------+----------+--------------------+-------------+\n",
      "|Abbotsford|85 Turner St|    2|   h|1480000|     S| Biggin|3/12/2016|     2.5|    3067|       2|       1|  1|     202|        null|     null|Yarra City Council| -37.7996|  144.9984|Northern Metropol...|         4019|\n",
      "|Abbotsford|16 Maugie St|    4|   h|   null|    SN| Nelson| 6/8/2016|     2.5|    3067|       3|       2|  2|     400|         220|     2006|Yarra City Council| -37.7965|  144.9965|Northern Metropol...|         4019|\n",
      "+----------+------------+-----+----+-------+------+-------+---------+--------+--------+--------+--------+---+--------+------------+---------+------------------+---------+----------+--------------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter by landsize > 200\n",
    "df.filter(df[\"Landsize\"] > 200).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Grouping and Aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|Rooms|        avg(Price)|\n",
      "+-----+------------------+\n",
      "|    1| 496595.2380952381|\n",
      "|    6|2891666.6666666665|\n",
      "|    3| 1106181.865284974|\n",
      "|    5|1627444.4444444445|\n",
      "|    4|1572198.1132075472|\n",
      "|    7|              null|\n",
      "|    2| 830115.2542372881|\n",
      "+-----+------------------+\n",
      "\n",
      "Time taken in seconds:  0.986379861831665\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df.groupBy(\"Rooms\").avg(\"Price\").show()\n",
    "end = time.time()\n",
    "print(\"Time taken in seconds: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m UnitPriceUDF \u001b[39m=\u001b[39m udf(FindUnitPriceK, FloatType())\n\u001b[0;32m     10\u001b[0m \u001b[39m# apply the function to create a new column\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mUnitPriceK\u001b[39m\u001b[39m\"\u001b[39m, UnitPriceUDF(df[\u001b[39m\"\u001b[39;49m\u001b[39mPrice\u001b[39;49m\u001b[39m\"\u001b[39;49m], df[\u001b[39m\"\u001b[39;49m\u001b[39mLandsize\u001b[39;49m\u001b[39m\"\u001b[39;49m]))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\udf.py:421\u001b[0m, in \u001b[0;36mUserDefinedFunction._wrapped.<locals>.wrapper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc, assigned\u001b[39m=\u001b[39massignments)\n\u001b[0;32m    420\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs: \u001b[39m\"\u001b[39m\u001b[39mColumnOrName\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Column:\n\u001b[1;32m--> 421\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\udf.py:398\u001b[0m, in \u001b[0;36mUserDefinedFunction.__call__\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m    396\u001b[0m         sc\u001b[39m.\u001b[39mprofiler_collector\u001b[39m.\u001b[39madd_profiler(\u001b[39mid\u001b[39m, memory_profiler)\n\u001b[0;32m    397\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 398\u001b[0m     judf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_judf\n\u001b[0;32m    399\u001b[0m     jPythonUDF \u001b[39m=\u001b[39m judf\u001b[39m.\u001b[39mapply(_to_seq(sc, cols, _to_java_column))\n\u001b[0;32m    400\u001b[0m \u001b[39mreturn\u001b[39;00m Column(jPythonUDF)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\udf.py:320\u001b[0m, in \u001b[0;36mUserDefinedFunction._judf\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_judf\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JavaObject:\n\u001b[0;32m    315\u001b[0m     \u001b[39m# It is possible that concurrent access, to newly created UDF,\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[39m# will initialize multiple UserDefinedPythonFunctions.\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     \u001b[39m# This is unlikely, doesn't affect correctness,\u001b[39;00m\n\u001b[0;32m    318\u001b[0m     \u001b[39m# and should have a minimal performance impact.\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_judf_placeholder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_judf_placeholder \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_judf(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc)\n\u001b[0;32m    321\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_judf_placeholder\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\udf.py:329\u001b[0m, in \u001b[0;36mUserDefinedFunction._create_judf\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    326\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39m_getActiveSessionOrCreate()\n\u001b[0;32m    327\u001b[0m sc \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39msparkContext\n\u001b[1;32m--> 329\u001b[0m wrapped_func \u001b[39m=\u001b[39m _wrap_function(sc, func, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturnType)\n\u001b[0;32m    330\u001b[0m jdt \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39m_jsparkSession\u001b[39m.\u001b[39mparseDataType(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturnType\u001b[39m.\u001b[39mjson())\n\u001b[0;32m    331\u001b[0m \u001b[39massert\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\udf.py:60\u001b[0m, in \u001b[0;36m_wrap_function\u001b[1;34m(sc, func, returnType)\u001b[0m\n\u001b[0;32m     58\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[39m=\u001b[39m _prepare_for_python_RDD(sc, command)\n\u001b[0;32m     59\u001b[0m \u001b[39massert\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[39mreturn\u001b[39;00m sc\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mSimplePythonFunction(\n\u001b[0;32m     61\u001b[0m     \u001b[39mbytearray\u001b[39;49m(pickled_command),\n\u001b[0;32m     62\u001b[0m     env,\n\u001b[0;32m     63\u001b[0m     includes,\n\u001b[0;32m     64\u001b[0m     sc\u001b[39m.\u001b[39;49mpythonExec,\n\u001b[0;32m     65\u001b[0m     sc\u001b[39m.\u001b[39;49mpythonVer,\n\u001b[0;32m     66\u001b[0m     broadcast_vars,\n\u001b[0;32m     67\u001b[0m     sc\u001b[39m.\u001b[39;49m_javaAccumulator,\n\u001b[0;32m     68\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "# Create custom function\n",
    "def FindUnitPriceK(price, landsize):\n",
    "    return price/1000/landsize\n",
    "\n",
    "# Register the function as a UDF\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "UnitPriceUDF = udf(FindUnitPriceK, FloatType())\n",
    "\n",
    "# apply the function to create a new column\n",
    "df = df.withColumn(\"UnitPriceK\", UnitPriceUDF(df[\"Price\"], df[\"Landsize\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
